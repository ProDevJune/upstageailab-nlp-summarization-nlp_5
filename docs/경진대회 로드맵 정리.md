#dialogSum #TextSummarization

# 1. 경진대회 개요 및 데이터 분석
### 1-1. 경진대회 목표 및 태스크 정의
- 데이터셋이 "한국어로 번역된" 버전이라는 점은 중요한 의미를 지닌다. 번역 과정에서 구어체 표현, 관용구, 담화 표지어 등의 미묘한 뉘앙스가 완벽하게 전달되지 않아 원래의 의도나 정보 흐름이 변경될 수 있다
	-> 이러한 번역 과정의 특성은 사전 학습된 한국어 자연어 처리 모델이 **==이 번역된 데이터셋에 특화된 불일치나 노이즈==**에 직면할 수 있음을 시사
-  EDA 과정에서 번역 시 발생할 수 있는 **=="노이즈"를 처리하기 위한 정제 과정==**이 필요하다.

### 1-2. DialogSum 데이터셋 특성
- 학습 데이터: 총 12,457개의 대화-요약 쌍
- 테스트 데이터: 총 499개의 대화로 구성
- 요약문 개수: 각 대화에는 3개의 고유한 정답 요약문이 존재
	-> ==모델 학습의 관점에서는, 이러한 다양성을 모델이 학습하도록 노출시키는 것이 중요==
- 개인정보 마스킹(PII)
	-> 요약 규칙 3번은 "대화 내에 중요한 명명된 개체를 보존"해야 한다고 명시하고 있으며, 이는 ==마스킹된 PII 토큰이 요약문의 핵심 콘텐츠 요구 사항과 직접적으로 연결됨==을 의미
	- ==학습에 반영하는 방법==
		1. ==special_token으로 추가하여 고유한 토큰으로 관리==
		2. ==모델에 이 special_token이 더 중요한 존재임을 알려주어야 한다. -> 어텐션 메커니즘에서 더 높은 가중치를 부여하거나 요약문에 이 토큰을 복사하는 것을 우선시하는 복사 메커니즘을 통합하는 방법을 사용.==
### 1-3. 평가지표
- 평가 지표 : ROUGE-N
	- "토큰화 후"라는 조건은 매우 중요. ROUGE 점수는 토큰화 방식에 매우 민감.
		-> ==일반적인 한국어 토크나이저들을 실험하고 데이터셋에 대한 출력을 분석하여 평가 방식과 가장 잘 일치할 가능성이 있는 것을 선택==
	- 학습, 생성, 평가 전반에 걸쳐 일관된 토크나이저를 사용하는 것이 보고되는 성능을 극대화하는 데 필수적
### 1-4. 탐색적 데이터 분석(EDA) 결과
- 텍스트 길이 분포 : right-skewed & 요약문의 길이가 약 20% 수준.
	-  right-skewed : 모델이 확장된 컨텍스트를 처리하는 능력에 도전 과제를 제시함
		-> ==길이가 긴 대화문에 어떤 특징이 있는지 파악==하고 그에 맞는 효과적인 자르기(truncation) 및  패딩(padding) 전략을 사용
	- 요약문의 길이가 약 20% 제약 : 모델이 상당한 추상화와 압축을 수행해야 함
		-> =='요약 후 요약'== 전략 : 1차 요약문이 대화문의 길이의 20%를 초과하는 경우, 요약 모델을 다시 사용하여 2차 요약을 하는 전략.
- 대화 주제(topic) : 30가지가 넘은 주제. 주제별로 언어 스타일, 핵심 개체, 주요 정보 등이 상이함.
	1. ==모델에 주제에 대한 정보를 안내하기 위해 주제 임베딩을 통합하는 방법==
	2. ==모델이 주제를 동시에 예측하는 다중 작업 학습 (label로 summary와 함께 topic을 같이 출력하고 두 loss를 더해서 backpropagate)==
	3. ==주제별로 클러스터링하여 데이터 개수가 충분하다고 여겨지는 주제들은 별도의 모델 학습==
- 비격식적 언어 
	- ==일단 train dialogue 데이터에 비격식적 언어가 있는지부터 확인==
	- 약어를 원본 단어로 변환하는 방법 . e.g) BTW -> By the way
	- 이모티콘을 감정 표현 언어로 변환하는 방법. e.g.) '^-^' -> 'smiling face'


# 2. 팀 협업을 위한 전략적 로드맵

## 2-1. 데이터 이해 및 전처리 (기반 구축)
#### - 데이터 정제 및 정규화
- 오타 및 문법 오류 수정
- 텍스트 유사성 기반 중복 데이터 제거
	-> ==중복 데이터를 데이터 증강을 위해 활용==
- 불용어 처리 : 불필요한 특수문자 제거, 공백 정규화
#### - Coreference 처리
- 더 정교한 입력 표현 : ==전용 화자 임베딩 레이어 사용==
- 모델이 화자의 턴을 명시적으로 추적, 발화 간의 지시정보를 해결하도록 아키텍쳐 수정.
#### - PII 마스킹 통합
- tokenizer의 special token으로 추가한다.
#### - 비격식적 언어 처리
- 규칙 기반 대체 : 약어를 전체 형태로 매핑하는 규칙을 작성한다. e.g.) BTW -> By the way 로 변환하는 함수 작성
	- 논문에서 요약문에 '감정'도 중요하게 작용한다 했으므로, 이모티콘도 적절한 토큰으로 대체하는 것이 바람직할 것 같다.
#### - 정보 분산 처리
- =="Excuse me...", "Ummmm...", "Wait..."와 같은 방해 표현의 빈도와 맥락을 분석==
- ==제거하거나 적절히 대체==



# 3. 베이스라인
### 초기 베이스라인
- 초기 feature engineering : 기본 텍스트 외에도 전체 대화 길이, 대화 주제, turn 등을 보조 입력으로 통합하는 것을 고려.
### 고급 모델링
- 계층적 학습
- 데이터 증강
	- 의역 : 한국어 -> 영어 -> 한국어로 다양성 증가
	- 개체 대체: 도일한 유형의 명명된 개체를 다른 것으로 대체
	- 턴 재정렬 : 대화의 논리적 흐름이 허용하는 겨웅 턴을 재정렬하여 데이터를 증강시킨다.
- QA 데이터셋 전이 학습 또는 DialogSum 벤치마크 상위 모델 전이 학습
-  DialogSum 논문의 오류 분석 기반
	- 화자 임베딩, 명시적 지시 정보 모듈
	- 복사 메커니즘
	- 핵심 정보 누락 : 핵심 정보 누락 시 더 큰 loss를 줄 수 있는 loss function 조사.
	- 사실 일관성 검사 후처리
	- 요약문 중복 : 생성된 요약문에 중복되는 문장을 감지하여 후처리 해야 함.


# Gemini-CLI 프롬프트
```

Generate a Python project for a Korean Dialogue Summarization competition. The task is abstractive summarization of multi-turn dialogues. Dataset: Korean-translated DialogSum (assume CSV format with 'id', 'dialogue', 'summary' and 'topic' columns). Evaluation Metric: mean of ROUGE-1, ROUGE-2, ROUGE-L scores. Implement a `DataLoader` class that loads data from specified file paths. For each dialogue, prepend speaker tags (`#Person1#`, `#Person2#`, etc.) to their respective utterances. Ensure `\n` acts as a turn separator. Example: `#Person1#: 안녕하세요.\n#Person2#: 네, 안녕하세요.`
Add the following 8 special tokens to the tokenizer's vocabulary and ensure they are treated as single tokens: `#PhoneNumber#`, `#Address#`, `#DateOfBirth#`, `#PassportNumber#`, `#SSN#`, `#CardNumber#`, `#CarNumber#`, `#Email#`. Include a placeholder function or comment for handling potential abbreviations and emoticons in the dialogue text, noting the need for normalization or removal. Use `AutoTokenizer.from_pretrained()` with the chosen base model's identifier. Ensure the added special tokens are correctly passed to `tokenizer.add_special_tokens()`. Set `max_input_length` to 1024 tokens and `max_output_length` to 256 tokens. Implement truncation for inputs exceeding `max_input_length` and padding to `max_input_length`. Load the specified pre-trained model for summarization/causal language modeling. Implement a standard fine-tuning loop using Hugging Face `Trainer`. Use `AdamW` optimizer. Include ROUGE metric calculation (using `evaluate` library) during evaluation. Save best model checkpoint based on validation ROUGE-L F1. 
Project Structure: - `main.py`: Main script for training and evaluation. - `data_loader.py`: Handles data loading, preprocessing, and `Dataset` creation. - `model.py`: For model initialization and specific architectural configurations. - `utils.py`: Contains helper functions, metric computation, and potentially custom callbacks. - `config.py`: Stores hyperparameters and model paths. - `requirements.txt`: Lists all necessary Python packages. 
Choose ONE of the following architectural options: 
--- OPTION A: Encoder-Decoder Architecture 
--- Base model: `monologg/kobart-base-v2` (KoBART) from Hugging Face Transformers. Use `AutoModelForSeq2SeqLM`. Configure `Seq2SeqTrainingArguments` and `DataCollatorForSeq2Seq`. Ensure the decoder uses `decoder_start_token_id` (typically `tokenizer.bos_token_id` or `tokenizer.pad_token_id`). 
--- OPTION B: Decoder-Only Architecture ---
Base model: `EleutherAI/polyglot-ko-1.3b` from Hugging Face Transformers. Use `AutoModelForCausalLM`. Configure `TrainingArguments` and `DataCollatorForLanguageModeling`. Format input as: ` {dialogue_text} {summary_text}`. Add ``, ``, `` as special tokens to the tokenizer. Ensure the loss calculation correctly masks the prompt part and only computes loss on the summary part. For inference, generate text by prompting with ` {dialogue_text}`.

```