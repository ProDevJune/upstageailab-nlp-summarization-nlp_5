# NLP í”„ë¡œì íŠ¸ ì‹¤í—˜ ìë™í™” ì‹œìŠ¤í…œ ì„¤ê³„ ë¬¸ì„œ

## ê°œìš”

ë³¸ ë¬¸ì„œëŠ” 6ì¡° CV í”„ë¡œì íŠ¸ì˜ ì„±ê³µì ì¸ ì‹¤í—˜ ìë™í™” ì „ëµì„ ë°”íƒ•ìœ¼ë¡œ, í˜„ì¬ NLP ìš”ì•½ í”„ë¡œì íŠ¸ì— ì í•©í•œ ìë™í™” ì‹œìŠ¤í…œì„ ì„¤ê³„í•œ ê²°ê³¼ì…ë‹ˆë‹¤. ê¸°ì¡´ íŒ€ì›ë“¤ì˜ ì›Œí¬í”Œë¡œìš°ë¥¼ ë°©í•´í•˜ì§€ ì•Šìœ¼ë©´ì„œ ì ì§„ì ìœ¼ë¡œ ì‹¤í—˜ ìë™í™”ë¥¼ ë„ì…í•  ìˆ˜ ìˆëŠ” êµ¬ì¡°ë¥¼ ì œì‹œí•©ë‹ˆë‹¤.

## 1. í˜„ì¬ NLP í”„ë¡œì íŠ¸ êµ¬ì¡° ë¶„ì„

### 1.1 ê¸°ì¡´ êµ¬ì¡°ì˜ ê°•ì 
```
nlp-sum-lyj/code/
â”œâ”€â”€ config.yaml          # ì˜ êµ¬ì¡°í™”ëœ ì„¤ì • íŒŒì¼
â”œâ”€â”€ baseline.ipynb       # ì™„ì „í•œ í•™ìŠµ íŒŒì´í”„ë¼ì¸
â”œâ”€â”€ solar_api.ipynb     # Solar API í™œìš© ë°©ì•ˆ
â””â”€â”€ requirements.txt     # ì˜ì¡´ì„± ê´€ë¦¬
```

**ê°•ì  ë¶„ì„:**
- âœ… **ê³„ì¸µì  ì„¤ì • êµ¬ì¡°**: general, tokenizer, training, wandb, inferenceë¡œ ì²´ê³„ì  ë¶„ë¥˜
- âœ… **NLP íŠ¹í™” ê³ ë ¤**: íŠ¹ìˆ˜ í† í°, ìƒì„± íŒŒë¼ë¯¸í„°, ROUGE í‰ê°€ ë“± í¬í•¨
- âœ… **WandB í†µí•© ì¤€ë¹„**: ì´ë¯¸ wandb ì„¤ì • ì„¹ì…˜ ì¡´ì¬
- âœ… **ì‹¤ìš©ì  íŒŒë¼ë¯¸í„°**: ëŒ€íšŒ íŠ¹ì„±ì— ë§ëŠ” í˜„ì‹¤ì  ì„¤ì •ê°’

### 1.2 í™•ì¥ í•„ìš”ì„± ë¶„ì„
```yaml
# í˜„ì¬ config.yamlì˜ í•œê³„ì 
âŒ WandB Sweep ì„¤ì • ë¶€ì¬
âŒ í•˜ì´í¼íŒŒë¼ë¯¸í„° íƒìƒ‰ ê³µê°„ ì •ì˜ ì—†ìŒ
âŒ ëª¨ë¸ë³„ íŠ¹í™” ì„¤ì • ë¶„ë¦¬ ì—†ìŒ
âŒ ìë™í™” ì‹¤í—˜ì„ ìœ„í•œ ë©”íƒ€ ì„¤ì • ë¶€ì¡±
âŒ ë² ì´ì§€ì•ˆ ìµœì í™” ì§€ì› ì—†ìŒ
```

## 2. í™•ì¥ ì„¤ê³„ ì „ëµ

### 2.1 ì ì§„ì  í™•ì¥ ì›ì¹™
1. **ê¸°ì¡´ êµ¬ì¡° ë³´ì¡´**: config.yamlê³¼ baseline.ipynbëŠ” ë³€ê²½ ì—†ì´ ìœ ì§€
2. **ì„ íƒì  ì‚¬ìš©**: ê¸°ì¡´ ë°©ì‹ê³¼ ìƒˆë¡œìš´ ìë™í™” ë°©ì‹ ë³‘í–‰ ì§€ì›
3. **í•˜ìœ„ í˜¸í™˜ì„±**: ê¸°ì¡´ ì„¤ì • íŒŒì¼ ì™„ì „ í˜¸í™˜
4. **ë‹¨ê³„ì  ë„ì…**: íŒ€ì›ë³„ë¡œ ì›í•˜ëŠ” ì‹œì ì— ìƒˆ ê¸°ëŠ¥ í™œìš©

### 2.2 í™•ì¥ëœ í´ë” êµ¬ì¡°
```
nlp-sum-lyj/
â”œâ”€â”€ code/
â”‚   â”œâ”€â”€ config.yaml                    # ê¸°ì¡´ ìœ ì§€ (ë³€ê²½ ì—†ìŒ)
â”‚   â”œâ”€â”€ baseline.ipynb                 # ê¸°ì¡´ ìœ ì§€ (ë³€ê²½ ì—†ìŒ)
â”‚   â”œâ”€â”€ solar_api.ipynb               # ê¸°ì¡´ ìœ ì§€
â”‚   â”œâ”€â”€ requirements.txt              # ê¸°ì¡´ ìœ ì§€
â”‚   â”‚
â”‚   â”œâ”€â”€ config/                       # ìƒˆë¡œìš´ í™•ì¥ ì„¤ì •
â”‚   â”‚   â”œâ”€â”€ base_config.yaml          # config.yaml í™•ì¥ ë²„ì „
â”‚   â”‚   â”œâ”€â”€ sweep/                    # WandB Sweep ì„¤ì •ë“¤
â”‚   â”‚   â”‚   â”œâ”€â”€ hyperparameter_sweep.yaml
â”‚   â”‚   â”‚   â”œâ”€â”€ model_comparison_sweep.yaml
â”‚   â”‚   â”‚   â”œâ”€â”€ ablation_study_sweep.yaml
â”‚   â”‚   â”‚   â””â”€â”€ quick_test_sweep.yaml
â”‚   â”‚   â””â”€â”€ models/                   # ëª¨ë¸ë³„ íŠ¹í™” ì„¤ì •
â”‚   â”‚       â”œâ”€â”€ kobart.yaml
â”‚   â”‚       â”œâ”€â”€ kogpt2.yaml
â”‚   â”‚       â”œâ”€â”€ t5.yaml
â”‚   â”‚       â””â”€â”€ solar_api.yaml
â”‚   â”‚
â”‚   â”œâ”€â”€ utils/                        # ìœ í‹¸ë¦¬í‹° ëª¨ë“ˆë“¤
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ config_manager.py         # ì„¤ì • ë¡œë”© ë° ê´€ë¦¬
â”‚   â”‚   â”œâ”€â”€ data_utils.py            # ë°ì´í„° ì²˜ë¦¬ ìœ í‹¸ë¦¬í‹°
â”‚   â”‚   â”œâ”€â”€ metrics.py               # ROUGE í‰ê°€ ì‹œìŠ¤í…œ
â”‚   â”‚   â””â”€â”€ experiment_utils.py      # ì‹¤í—˜ ê´€ë¦¬ ë„êµ¬
â”‚   â”‚
â”‚   â”œâ”€â”€ modules/                      # ëª¨ë“ˆí™”ëœ ì»´í¬ë„ŒíŠ¸
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ data_module.py           # ë°ì´í„°ì…‹ ë° ì „ì²˜ë¦¬
â”‚   â”‚   â”œâ”€â”€ model_module.py          # ëª¨ë¸ ë¡œë”© ë° ì„¤ì •
â”‚   â”‚   â””â”€â”€ trainer_module.py        # í•™ìŠµ ë¡œì§
â”‚   â”‚
â”‚   â”œâ”€â”€ trainer.py                    # baseline.ipynb ëª¨ë“ˆí™” ë²„ì „
â”‚   â”œâ”€â”€ sweep_runner.py              # WandB Sweep ì‹¤í–‰ê¸° (6ì¡° ë°©ì‹)
â”‚   â”œâ”€â”€ inference.py                 # ì¶”ë¡  ì „ìš© ëª¨ë“ˆ
â”‚   â””â”€â”€ experiment_runner.py         # í†µí•© ì‹¤í—˜ ì‹¤í–‰ê¸°
â”‚
â””â”€â”€ docs/experiment_automation/       # ë¬¸ì„œí™”
    â”œâ”€â”€ README.md
    â”œâ”€â”€ cv_team6_analysis.md         # 6ì¡° ë¶„ì„ (ì™„ë£Œ)
    â”œâ”€â”€ nlp_automation_design.md     # ë³¸ ë¬¸ì„œ
    â”œâ”€â”€ configuration_guide.md
    â””â”€â”€ best_practices.md
```

## 3. ì„¤ì • íŒŒì¼ í™•ì¥ ì„¤ê³„

### 3.1 base_config.yaml (ê¸°ì¡´ config.yaml í™•ì¥)
```yaml
# ê¸°ì¡´ config.yamlì˜ ëª¨ë“  ì„¤ì •ì„ í¬í•¨í•˜ë˜ í™•ì¥
meta:
  experiment_name: "nlp_summarization"
  version: "1.0"
  description: "Dialogue Summarization with Automated Hyperparameter Tuning"
  
general:
  data_path: "../data/"
  model_name: "digit82/kobart-summarization"
  output_dir: "./"
  seed: 42
  device: "auto"  # auto, cuda, cpu

model:
  architecture: "kobart"  # kobart, kogpt2, t5, solar_api
  checkpoint: "digit82/kobart-summarization"
  load_pretrained: true

tokenizer:
  encoder_max_len: 512
  decoder_max_len: 100
  bos_token: "<s>"
  eos_token: "</s>"
  special_tokens: ['#Person1#', '#Person2#', '#Person3#', '#PhoneNumber#', '#Address#', '#PassportNumber#']
  truncation: true
  padding: "max_length"

training:
  # ê¸°ì¡´ training ì„¤ì • ëª¨ë‘ í¬í•¨
  overwrite_output_dir: true
  num_train_epochs: 20
  learning_rate: 1.0e-05
  per_device_train_batch_size: 50
  per_device_eval_batch_size: 32
  warmup_ratio: 0.1
  weight_decay: 0.01
  lr_scheduler_type: 'cosine'
  optim: 'adamw_torch'
  gradient_accumulation_steps: 1
  evaluation_strategy: 'epoch'
  save_strategy: 'epoch'
  save_total_limit: 5
  fp16: true
  load_best_model_at_end: true
  logging_dir: "./logs"
  logging_strategy: "epoch"
  predict_with_generate: true
  generation_max_length: 100
  do_train: true
  do_eval: true
  early_stopping_patience: 3
  early_stopping_threshold: 0.001
  report_to: "wandb"
  
  # ìƒˆë¡œìš´ NLP íŠ¹í™” ì„¤ì • ì¶”ê°€
  label_smoothing: 0.0
  dataloader_num_workers: 4
  remove_unused_columns: false

generation:  # ì¶”ë¡  ì„¤ì •ì„ ë³„ë„ ì„¹ì…˜ìœ¼ë¡œ ë¶„ë¦¬
  max_length: 100
  min_length: 10
  num_beams: 4
  no_repeat_ngram_size: 2
  early_stopping: true
  length_penalty: 1.0
  repetition_penalty: 1.0
  do_sample: false
  temperature: 1.0
  top_k: 50
  top_p: 1.0

evaluation:
  metrics: ["rouge1", "rouge2", "rougeL"]
  multi_reference: true  # ëŒ€íšŒ íŠ¹ì„± (3ê°œ ì •ë‹µ)
  rouge_use_stemmer: true
  rouge_tokenize_korean: true

wandb:
  entity: "wandb_repo"
  project: "nlp-summarization-auto"
  name: "auto_experiment"
  notes: "Automated hyperparameter tuning experiment"
  tags: ["nlp", "summarization", "automl"]

inference:
  ckt_path: "model_ckt_path"
  result_path: "./prediction/"
  batch_size: 32
  remove_tokens: ['<usr>', '<s>', '</s>', '<pad>']
  output_format: "csv"  # csv, json
```

### 3.2 WandB Sweep ì„¤ì • (sweep/hyperparameter_sweep.yaml)
```yaml
# 6ì¡° ë°©ì‹ì„ NLPì— ì ìš©í•œ ë² ì´ì§€ì•ˆ ìµœì í™” ì„¤ì •
project: nlp-summarization-auto
entity: wandb_repo
program: sweep_runner.py

method: bayes  # ë² ì´ì§€ì•ˆ ìµœì í™”
metric:
  name: rouge_combined_f1  # ROUGE-1 + ROUGE-2 + ROUGE-L í•©ê³„
  goal: maximize

# ì¡°ê¸° ì¢…ë£Œ ì„¤ì •
early_terminate:
  type: hyperband
  min_iter: 5
  max_iter: 20

parameters:
  # í•™ìŠµë¥  ê´€ë ¨
  learning_rate:
    distribution: log_uniform_values
    min: 1.0e-6
    max: 1.0e-4
  
  # ë°°ì¹˜ í¬ê¸°
  per_device_train_batch_size:
    values: [8, 16, 32, 64]
  
  per_device_eval_batch_size:
    values: [16, 32, 64]
  
  # í† í° ê¸¸ì´ ì„¤ì • (NLP íŠ¹í™”)
  encoder_max_len:
    values: [256, 512, 1024]
  
  decoder_max_len:
    values: [64, 100, 128, 256]
  
  # ì •ê·œí™” íŒŒë¼ë¯¸í„°
  weight_decay:
    distribution: log_uniform_values
    min: 1.0e-3
    max: 1.0e-1
  
  warmup_ratio:
    values: [0.0, 0.1, 0.2, 0.3]
  
  # ìƒì„± íŒŒë¼ë¯¸í„° (NLP íŠ¹í™”)
  num_beams:
    values: [3, 4, 5, 8]
  
  no_repeat_ngram_size:
    values: [2, 3, 4]
  
  length_penalty:
    distribution: uniform
    min: 0.8
    max: 1.5
  
  # í•™ìŠµ ì„¤ì •
  num_train_epochs:
    values: [10, 15, 20, 25]
  
  gradient_accumulation_steps:
    values: [1, 2, 4]
  
  # ì •ê·œí™”
  label_smoothing:
    values: [0.0, 0.1, 0.2]
  
  # ì¡°ê¸° ì¢…ë£Œ
  early_stopping_patience:
    values: [3, 5, 7]

# ì‹¤í—˜ ì œì•½ ì¡°ê±´
constraints:
  # ë©”ëª¨ë¦¬ ì œì•½ ê³ ë ¤
  - batch_size_memory_constraint:
      if: encoder_max_len > 512
      then: per_device_train_batch_size <= 16
```

### 3.3 ëª¨ë¸ë³„ íŠ¹í™” ì„¤ì • (models/kobart.yaml)
```yaml
# KoBART íŠ¹í™” ì„¤ì •
model:
  architecture: "kobart"
  checkpoint: "digit82/kobart-summarization"
  config_overrides:
    max_position_embeddings: 1024
    vocab_size: 30000

tokenizer:
  model_max_length: 1024
  special_tokens: ['#Person1#', '#Person2#', '#Person3#', '#PhoneNumber#', '#Address#', '#PassportNumber#']

training:
  # KoBARTì— ìµœì í™”ëœ ê¸°ë³¸ê°’
  learning_rate: 3.0e-05
  per_device_train_batch_size: 32
  warmup_ratio: 0.1
  weight_decay: 0.01
  
generation:
  max_length: 128
  num_beams: 5
  length_penalty: 1.2
  no_repeat_ngram_size: 3

# KoBART íŠ¹í™” í•˜ì´í¼íŒŒë¼ë¯¸í„° íƒìƒ‰ ê³µê°„
sweep_parameters:
  learning_rate:
    min: 1.0e-5
    max: 5.0e-5
  length_penalty:
    min: 0.9
    max: 1.5
```

## 4. ëª¨ë“ˆí™” ì„¤ê³„

### 4.1 baseline.ipynb ëª¨ë“ˆí™” ì „ëµ

**ê¸°ì¡´ baseline.ipynbì˜ ì£¼ìš” ì„¹ì…˜ë“¤:**
1. **ë°ì´í„° ì „ì²˜ë¦¬** â†’ `modules/data_module.py`
2. **ëª¨ë¸ ë¡œë”© ë° ì„¤ì •** â†’ `modules/model_module.py`
3. **í•™ìŠµ ë£¨í”„** â†’ `modules/trainer_module.py`
4. **í‰ê°€ ë° ë©”íŠ¸ë¦­** â†’ `utils/metrics.py`
5. **ì¶”ë¡ ** â†’ `inference.py`

### 4.2 ConfigManager ì„¤ê³„ (utils/config_manager.py)
```python
class ConfigManager:
    """ì„¤ì • íŒŒì¼ í†µí•© ê´€ë¦¬ì - ê¸°ì¡´/ì‹ ê·œ ì„¤ì • ëª¨ë‘ ì§€ì›"""
    
    def __init__(self):
        self.config = None
        self.is_legacy = False
    
    def load_config(self, config_path):
        """ê¸°ì¡´ config.yamlê³¼ ìƒˆë¡œìš´ base_config.yaml ëª¨ë‘ ì§€ì›"""
        with open(config_path, 'r') as f:
            config = yaml.safe_load(f)
        
        # ê¸°ì¡´ config.yaml í˜•ì‹ ê°ì§€ ë° ìë™ ë³€í™˜
        if self._is_legacy_format(config):
            config = self._migrate_legacy_config(config)
            self.is_legacy = True
        
        self.config = config
        return config
    
    def merge_sweep_config(self, sweep_params):
        """WandB Sweep íŒŒë¼ë¯¸í„°ë¥¼ ë™ì ìœ¼ë¡œ ë³‘í•©"""
        # 6ì¡° ë°©ì‹ê³¼ ë™ì¼í•œ ë™ì  ì„¤ì • ì—…ë°ì´íŠ¸
        pass
    
    def _is_legacy_format(self, config):
        """ê¸°ì¡´ config.yaml í˜•ì‹ì¸ì§€ íŒë‹¨"""
        return "meta" not in config and "general" in config
    
    def _migrate_legacy_config(self, legacy_config):
        """ê¸°ì¡´ ì„¤ì •ì„ ìƒˆë¡œìš´ í˜•ì‹ìœ¼ë¡œ ìë™ ë³€í™˜"""
        # ê¸°ì¡´ íŒ€ì›ë“¤ì˜ config.yamlì„ ìƒˆ í˜•ì‹ìœ¼ë¡œ íˆ¬ëª…í•˜ê²Œ ë³€í™˜
        pass
```

### 4.3 NLP íŠ¹í™” ë©”íŠ¸ë¦­ ì‹œìŠ¤í…œ (utils/metrics.py)
```python
class MultiReferenceROUGE:
    """ëŒ€íšŒ íŠ¹ì„±ì— ë§ëŠ” Multi-reference ROUGE í‰ê°€"""
    
    def __init__(self):
        self.rouge = Rouge()
    
    def compute_metrics(self, eval_preds):
        """HuggingFace Trainerì™€ í˜¸í™˜ë˜ëŠ” ë©”íŠ¸ë¦­ ê³„ì‚°"""
        predictions, labels = eval_preds
        
        # 3ê°œ ì°¸ì¡° ìš”ì•½ë¬¸ ì¤‘ ìµœê³  ì ìˆ˜ ê³„ì‚°
        rouge_scores = self._compute_multi_reference_rouge(predictions, labels)
        
        return {
            "rouge1_f1": rouge_scores["rouge1_f1"],
            "rouge2_f1": rouge_scores["rouge2_f1"],
            "rougeL_f1": rouge_scores["rougeL_f1"],
            "rouge_combined_f1": sum(rouge_scores.values())  # WandB Sweep ëª©í‘œ
        }
    
    def _compute_multi_reference_rouge(self, predictions, references):
        """ê° ì˜ˆì¸¡ì— ëŒ€í•´ 3ê°œ ì°¸ì¡° ì¤‘ ìµœê³  ì ìˆ˜ ë°˜í™˜"""
        pass
```

## 5. ìë™í™” ìŠ¤í¬ë¦½íŠ¸ ì„¤ê³„

### 5.1 sweep_runner.py (6ì¡° ë°©ì‹ ì ìš©)
```python
import wandb
from utils.config_manager import ConfigManager
from modules.trainer_module import NLPTrainer
from utils.metrics import MultiReferenceROUGE

def train():
    """WandB Sweepì—ì„œ í˜¸ì¶œë˜ëŠ” ë©”ì¸ í›ˆë ¨ í•¨ìˆ˜"""
    # 1. ì„¤ì • ë¡œë”© (6ì¡° ë°©ì‹)
    config_manager = ConfigManager()
    base_config = config_manager.load_config("config/base_config.yaml")
    
    # 2. WandB Sweep íŒŒë¼ë¯¸í„° ë™ì  ë³‘í•©
    wandb_config = wandb.config
    config = config_manager.merge_sweep_config(wandb_config)
    
    # 3. NLP íŠ¹í™” ëª¨ë¸ ë° ë°ì´í„° ì„¤ì •
    trainer = NLPTrainer(config)
    
    # 4. Multi-reference ROUGE í‰ê°€ ì„¤ì •
    metrics = MultiReferenceROUGE()
    
    # 5. í•™ìŠµ ì‹¤í–‰
    results = trainer.train()
    
    # 6. ëŒ€íšŒ íŠ¹ì„±ì— ë§ëŠ” ìµœì¢… ì ìˆ˜ ê³„ì‚°
    final_score = results["rouge_combined_f1"]
    wandb.log({"final_rouge_score": final_score})
    
    return results

if __name__ == "__main__":
    # 6ì¡° ë°©ì‹ê³¼ ë™ì¼í•œ WandB Sweep ì‹¤í–‰
    sweep_config = yaml.safe_load(open("config/sweep/hyperparameter_sweep.yaml"))
    sweep_id = wandb.sweep(sweep=sweep_config, project="nlp-summarization-auto")
    wandb.agent(sweep_id=sweep_id, function=train, count=20)
```

### 5.2 experiment_runner.py (í†µí•© ì‹¤í—˜ ê´€ë¦¬)
```python
class ExperimentRunner:
    """ë‹¤ì–‘í•œ ì‹¤í—˜ íƒ€ì…ì„ í†µí•© ê´€ë¦¬í•˜ëŠ” ì‹¤í–‰ê¸°"""
    
    def __init__(self):
        self.config_manager = ConfigManager()
    
    def run_single_experiment(self, config_path):
        """ë‹¨ì¼ ì‹¤í—˜ ì‹¤í–‰ (ê¸°ì¡´ ë°©ì‹ê³¼ í˜¸í™˜)"""
        pass
    
    def run_hyperparameter_sweep(self, sweep_config_path):
        """í•˜ì´í¼íŒŒë¼ë¯¸í„° íƒìƒ‰ ì‹¤í—˜"""
        pass
    
    def run_model_comparison(self, models_list):
        """ì—¬ëŸ¬ ëª¨ë¸ ì„±ëŠ¥ ë¹„êµ ì‹¤í—˜"""
        pass
    
    def run_ablation_study(self, ablation_config):
        """ì†Œê±° ì—°êµ¬ ì‹¤í—˜"""
        pass
```

## 6. í˜¸í™˜ì„± ë³´ì¥ ì „ëµ

### 6.1 ê¸°ì¡´ ì›Œí¬í”Œë¡œìš° ë³´ì¡´
```python
# ê¸°ì¡´ ë°©ì‹ (ë³€ê²½ ì—†ìŒ)
with open("config.yaml", "r") as f:
    config = yaml.safe_load(f)
# ê¸°ì¡´ baseline.ipynb ì½”ë“œ ê·¸ëŒ€ë¡œ ì‚¬ìš© ê°€ëŠ¥

# ìƒˆë¡œìš´ ë°©ì‹ (ì„ íƒì  ì‚¬ìš©)
config_manager = ConfigManager()
config = config_manager.load_config("config/base_config.yaml")
# ë˜ëŠ” ê¸°ì¡´ config.yamlë„ ìë™ ë³€í™˜í•˜ì—¬ ì‚¬ìš©
config = config_manager.load_config("config.yaml")  # ìë™ ë³€í™˜ë¨
```

### 6.2 ì ì§„ì  ë§ˆì´ê·¸ë ˆì´ì…˜ ì§€ì›
1. **Phase 1**: ê¸°ì¡´ config.yaml + ìƒˆë¡œìš´ utils ëª¨ë“ˆ ì‚¬ìš©
2. **Phase 2**: ì¼ë¶€ ì‹¤í—˜ì—ë§Œ ìƒˆë¡œìš´ ì„¤ì • êµ¬ì¡° ì ìš©
3. **Phase 3**: ì›í•˜ëŠ” íŒ€ì›ë§Œ WandB Sweep í™œìš©
4. **Phase 4**: ì™„ì „ ìë™í™” ì „í™˜ (ì„ íƒ ì‚¬í•­)

## 7. NLP íŠ¹í™” ê³ ë ¤ì‚¬í•­

### 7.1 ëŒ€íšŒ íŠ¹ì„± ë°˜ì˜
- **Multi-reference í‰ê°€**: 3ê°œ ì •ë‹µ ìš”ì•½ë¬¸ ì¤‘ ìµœê³  ì ìˆ˜
- **í•œêµ­ì–´ í† í°í™”**: í˜•íƒœì†Œ ë¶„ì„ê¸° ê¸°ë°˜ ROUGE ê³„ì‚°
- **ìƒì„± ëª¨ë¸ íŒŒë¼ë¯¸í„°**: beam search, length penalty ë“± ìµœì í™”
- **Solar API í†µí•©**: ì™¸ë¶€ API ê¸°ë°˜ ì‹¤í—˜ë„ ìë™í™” ì§€ì›

### 7.2 ë©”ëª¨ë¦¬ ë° ì„±ëŠ¥ ìµœì í™”
```yaml
# ë©”ëª¨ë¦¬ ì œì•½ ê³ ë ¤ ì„¤ì •
constraints:
  memory_optimization:
    if: encoder_max_len > 512
    then: 
      per_device_train_batch_size: max 16
      gradient_accumulation_steps: min 2
  
  performance_optimization:
    dataloader_num_workers: 4
    pin_memory: true
    fp16: true
```

## 8. êµ¬í˜„ ìš°ì„ ìˆœìœ„

### 8.1 Phase 1: ê¸°ë°˜ êµ¬ì¡° êµ¬ì¶• (1ì£¼ì°¨)
1. **ConfigManager êµ¬í˜„**: ê¸°ì¡´/ì‹ ê·œ ì„¤ì • í˜¸í™˜ ì‹œìŠ¤í…œ
2. **base_config.yaml ì‘ì„±**: ê¸°ì¡´ config.yaml í™•ì¥
3. **ìœ í‹¸ë¦¬í‹° ëª¨ë“ˆ**: metrics.py, data_utils.py êµ¬í˜„
4. **í˜¸í™˜ì„± í…ŒìŠ¤íŠ¸**: ê¸°ì¡´ ì½”ë“œì™€ì˜ ë™ì‘ í™•ì¸

### 8.2 Phase 2: ëª¨ë“ˆí™” ë° ìë™í™” (2ì£¼ì°¨)
1. **baseline.ipynb ëª¨ë“ˆí™”**: trainer.py, modules/ êµ¬í˜„
2. **WandB Sweep ì„¤ì •**: sweep/ í´ë” ì„¤ì • íŒŒì¼ë“¤ ì‘ì„±
3. **sweep_runner.py êµ¬í˜„**: 6ì¡° ë°©ì‹ ìë™í™” ìŠ¤í¬ë¦½íŠ¸
4. **ì²« ë²ˆì§¸ ìë™í™” ì‹¤í—˜**: ê°„ë‹¨í•œ í•˜ì´í¼íŒŒë¼ë¯¸í„° íƒìƒ‰

### 8.3 Phase 3: ê³ ë„í™” ë° ìµœì í™” (3ì£¼ì°¨)
1. **experiment_runner.py**: í†µí•© ì‹¤í—˜ ê´€ë¦¬ ì‹œìŠ¤í…œ
2. **ëª¨ë¸ ë¹„êµ ìë™í™”**: ì—¬ëŸ¬ ëª¨ë¸ ì„±ëŠ¥ ë¹„êµ
3. **ê²°ê³¼ ë¶„ì„ ë„êµ¬**: ì‹¤í—˜ ê²°ê³¼ ìë™ ë¶„ì„ ë° ì‹œê°í™”
4. **Solar API í†µí•©**: ì™¸ë¶€ API ê¸°ë°˜ ì‹¤í—˜ ìë™í™”

## 9. ê¸°ëŒ€ íš¨ê³¼

### 9.1 ì¦‰ì‹œ íš¨ê³¼ (Phase 1 ì™„ë£Œ í›„)
- âœ… ì„¤ì • ê´€ë¦¬ ì²´ê³„í™”
- âœ… ì‹¤í—˜ ì¬í˜„ì„± ë³´ì¥
- âœ… íŒ€ í˜‘ì—… íš¨ìœ¨ì„± ì¦ëŒ€

### 9.2 ì¤‘ê¸° íš¨ê³¼ (Phase 2 ì™„ë£Œ í›„)
- ğŸš€ í•˜ì´í¼íŒŒë¼ë¯¸í„° ìë™ ìµœì í™”
- ğŸ“Š ì²´ê³„ì  ì‹¤í—˜ ì¶”ì  ë° ë¹„êµ
- â±ï¸ ì‹¤í—˜ ì‹œê°„ ë‹¨ì¶• (ìˆ˜ë™ â†’ ìë™)

### 9.3 ì¥ê¸° íš¨ê³¼ (Phase 3 ì™„ë£Œ í›„)
- ğŸ¯ ìµœì  ëª¨ë¸ ìë™ ë°œê²¬
- ğŸ”„ ì§€ì†ì  ì„±ëŠ¥ ê°œì„  ì‹œìŠ¤í…œ
- ğŸ“ˆ ëŒ€íšŒ ìˆœìœ„ í–¥ìƒ ê¸°ëŒ€

## 10. ê²°ë¡ 

ë³¸ ì„¤ê³„ëŠ” 6ì¡°ì˜ ì„±ê³µì ì¸ ì‹¤í—˜ ìë™í™” ì „ëµì„ NLP í”„ë¡œì íŠ¸ì— ë§ê²Œ ì ì‘ì‹œí‚¤ë©´ì„œë„, ê¸°ì¡´ íŒ€ì›ë“¤ì˜ ì›Œí¬í”Œë¡œìš°ë¥¼ ì¡´ì¤‘í•˜ëŠ” ì ì§„ì  ë„ì… ë°©ì‹ì„ ì œì‹œí•©ë‹ˆë‹¤. 

**í•µì‹¬ ê°€ì¹˜:**
- **í˜¸í™˜ì„±**: ê¸°ì¡´ ì½”ë“œ ì™„ì „ ë³´ì¡´
- **ì„ íƒì„±**: ì›í•˜ëŠ” ê¸°ëŠ¥ë§Œ ì„ íƒì  ì‚¬ìš©
- **í™•ì¥ì„±**: ë¯¸ë˜ ìš”êµ¬ì‚¬í•­ì— ìœ ì—°í•˜ê²Œ ëŒ€ì‘
- **ì‹¤ìš©ì„±**: ëŒ€íšŒ íŠ¹ì„±ê³¼ íŒ€ ìƒí™©ì— ìµœì í™”

ì´ë¥¼ í†µí•´ íŒ€ì˜ ì‹¤í—˜ íš¨ìœ¨ì„±ì„ í¬ê²Œ ë†’ì´ë©´ì„œë„ í•™ìŠµ ê³¡ì„ ì„ ìµœì†Œí™”í•  ìˆ˜ ìˆì„ ê²ƒìœ¼ë¡œ ê¸°ëŒ€ë©ë‹ˆë‹¤.
