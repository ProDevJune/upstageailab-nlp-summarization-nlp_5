# NLP ëŒ€í™” ìš”ì•½ í”„ë¡œì íŠ¸ - ì‹¤í—˜ ìë™í™” ì‹œìŠ¤í…œ ê°€ì´ë“œ

## ëª©ì°¨
1. [ì‹œìŠ¤í…œ ê°œìš”](#ì‹œìŠ¤í…œ-ê°œìš”)
2. [ì£¼ìš” ì¥ì ](#ì£¼ìš”-ì¥ì )
3. [ì‹œì‘í•˜ê¸°](#ì‹œì‘í•˜ê¸°)
4. [ë¹ ë¥¸ ì‹œì‘ ê°€ì´ë“œ](#ë¹ ë¥¸-ì‹œì‘-ê°€ì´ë“œ)
5. [ìƒì„¸ ì‚¬ìš©ë²•](#ìƒì„¸-ì‚¬ìš©ë²•)
6. [ì‹¤í—˜ ê²°ê³¼ ë¶„ì„](#ì‹¤í—˜-ê²°ê³¼-ë¶„ì„)
7. [ë¬¸ì œ í•´ê²°](#ë¬¸ì œ-í•´ê²°)
8. [ìì£¼ ë¬»ëŠ” ì§ˆë¬¸](#ìì£¼-ë¬»ëŠ”-ì§ˆë¬¸)

---

## ì‹œìŠ¤í…œ ê°œìš”

ìš°ë¦¬ íŒ€ì˜ ì‹¤í—˜ ìë™í™” ì‹œìŠ¤í…œì€ 6ì¡°ì˜ ì„±ê³µì ì¸ CV í”„ë¡œì íŠ¸ ì ‘ê·¼ë²•ì„ NLP ëŒ€í™” ìš”ì•½ íƒœìŠ¤í¬ì— ë§ê²Œ ê°œì„ í•œ ê²ƒì…ë‹ˆë‹¤. ì´ ì‹œìŠ¤í…œì„ í†µí•´ ë‹¤ìŒê³¼ ê°™ì€ ì‘ì—…ì„ ìë™í™”í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤:

- ğŸš€ **í•˜ì´í¼íŒŒë¼ë¯¸í„° ìµœì í™”**: í•™ìŠµë¥ , ë°°ì¹˜ í¬ê¸°, ì—í­ ìˆ˜ ë“±ì„ ìë™ìœ¼ë¡œ íŠœë‹
- ğŸ”¬ **ëª¨ë¸ ë¹„êµ ì‹¤í—˜**: KoBART, KoGPT2, T5, mT5 ë“± ë‹¤ì–‘í•œ ëª¨ë¸ ì„±ëŠ¥ ë¹„êµ
- ğŸ“Š **ì²´ê³„ì ì¸ ì‹¤í—˜ ê´€ë¦¬**: WandBë¥¼ í†µí•œ ì‹¤ì‹œê°„ ëª¨ë‹ˆí„°ë§ê³¼ ê²°ê³¼ ì¶”ì 
- âš¡ **ë³‘ë ¬ ì‹¤í—˜ ì‹¤í–‰**: ì—¬ëŸ¬ ì‹¤í—˜ì„ ë™ì‹œì— ì‹¤í–‰í•˜ì—¬ ì‹œê°„ ë‹¨ì¶•

### ì‹œìŠ¤í…œ ì•„í‚¤í…ì²˜

```
project_root/
â”œâ”€â”€ code/
â”‚   â”œâ”€â”€ config/                 # ì„¤ì • íŒŒì¼ë“¤
â”‚   â”‚   â”œâ”€â”€ base_config.yaml   # ê¸°ë³¸ ì„¤ì •
â”‚   â”‚   â”œâ”€â”€ models/            # ëª¨ë¸ë³„ ì„¤ì •
â”‚   â”‚   â””â”€â”€ sweep/             # Sweep ì„¤ì •
â”‚   â”œâ”€â”€ utils/                  # ìœ í‹¸ë¦¬í‹° ëª¨ë“ˆ
â”‚   â”œâ”€â”€ trainer.py             # í•™ìŠµ ëª¨ë“ˆ
â”‚   â”œâ”€â”€ sweep_runner.py        # Sweep ì‹¤í–‰ê¸°
â”‚   â””â”€â”€ scripts/               # ì‹¤í–‰ ìŠ¤í¬ë¦½íŠ¸
â”œâ”€â”€ data/                      # ë°ì´í„° íŒŒì¼
â””â”€â”€ outputs/                   # ì‹¤í—˜ ê²°ê³¼
```

---

## ì£¼ìš” ì¥ì 

### 1. ğŸ¯ **ê¸°ì¡´ ì›Œí¬í”Œë¡œìš°ì™€ì˜ ì™„ë²½í•œ í˜¸í™˜ì„±**
- baseline.ipynbì˜ ëª¨ë“  ê¸°ëŠ¥ì„ ê·¸ëŒ€ë¡œ ì‚¬ìš© ê°€ëŠ¥
- ê¸°ì¡´ config.yaml í˜•ì‹ ìë™ ë§ˆì´ê·¸ë ˆì´ì…˜
- ì ì§„ì  ë„ì… ê°€ëŠ¥ (ì¼ë¶€ ì‹¤í—˜ë§Œ ìë™í™”)

### 2. ğŸ”§ **ìœ ì—°í•œ ì„¤ì • ê´€ë¦¬**
- YAML ê¸°ë°˜ ê³„ì¸µì  ì„¤ì • ì‹œìŠ¤í…œ
- í™˜ê²½ë³€ìˆ˜ë¥¼ í†µí•œ ì˜¤ë²„ë¼ì´ë“œ ì§€ì›
- ëª¨ë¸ë³„, ì‹¤í—˜ë³„ ì„¤ì • ë¶„ë¦¬

### 3. ğŸ“ˆ **ê°•ë ¥í•œ ì‹¤í—˜ ì¶”ì **
- WandB ëŒ€ì‹œë³´ë“œì—ì„œ ì‹¤ì‹œê°„ ëª¨ë‹ˆí„°ë§
- ìë™ ë©”íŠ¸ë¦­ ë¡œê¹… (ROUGE-1, ROUGE-2, ROUGE-L)
- ìµœì  ëª¨ë¸ ìë™ ì €ì¥

### 4. â±ï¸ **ì‹œê°„ íš¨ìœ¨ì„±**
- ë³‘ë ¬ ì‹¤í—˜ ì‹¤í–‰ìœ¼ë¡œ ì‹œê°„ ë‹¨ì¶•
- ìë™ ì¬ì‹œì‘ ê¸°ëŠ¥ (ì¤‘ë‹¨ëœ ì‹¤í—˜ ì´ì–´ì„œ ì‹¤í–‰)
- ìŠ¤ë§ˆíŠ¸ ë©”ëª¨ë¦¬ ê´€ë¦¬

---

## ì‹œì‘í•˜ê¸°

### í™˜ê²½ ì„¤ì •

1. **í•„ìš”í•œ íŒ¨í‚¤ì§€ ì„¤ì¹˜**
```bash
pip install -r requirements.txt
```

ì£¼ìš” íŒ¨í‚¤ì§€:
- `transformers>=4.30.0`
- `wandb>=0.15.0`
- `torch>=2.0.0`
- `datasets>=2.10.0`
- `evaluate>=0.4.0`
- `rouge-score>=0.1.2`

2. **WandB ì„¤ì •**
```bash
# WandB ê³„ì • ë¡œê·¸ì¸
wandb login

# í”„ë¡œì íŠ¸ ì„¤ì • (ì„ íƒì‚¬í•­)
export WANDB_PROJECT="nlp-dialogue-summarization"
export WANDB_ENTITY="your-team-name"
```

3. **GPU í™•ì¸**
```python
import torch
print(f"GPU Available: {torch.cuda.is_available()}")
print(f"GPU Name: {torch.cuda.get_device_name(0)}")
```

---

## ë¹ ë¥¸ ì‹œì‘ ê°€ì´ë“œ

### ğŸ® ì¼€ì´ìŠ¤ 1: ê¸°ë³¸ í•™ìŠµ ì‹¤í–‰
ê¸°ì¡´ baseline.ipynbì²˜ëŸ¼ ë‹¨ì¼ ì‹¤í—˜ì„ ì‹¤í–‰í•˜ê³  ì‹¶ì„ ë•Œ:

```bash
cd code
python trainer.py \
    --config config/base_config.yaml \
    --train-data ../data/train.csv \
    --val-data ../data/dev.csv
```

### ğŸ”¬ ì¼€ì´ìŠ¤ 2: í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹
ìµœì ì˜ í•™ìŠµë¥ ê³¼ ë°°ì¹˜ í¬ê¸°ë¥¼ ì°¾ê³  ì‹¶ì„ ë•Œ:

```bash
cd code
python sweep_runner.py \
    --base-config config/base_config.yaml \
    --sweep-config hyperparameter_sweep \
    --count 20
```

### ğŸ† ì¼€ì´ìŠ¤ 3: ëª¨ë¸ ì„±ëŠ¥ ë¹„êµ
ì—¬ëŸ¬ ëª¨ë¸ì˜ ì„±ëŠ¥ì„ ë¹„êµí•˜ê³  ì‹¶ì„ ë•Œ:

```bash
cd code
python sweep_runner.py \
    --base-config config/base_config.yaml \
    --sweep-config model_comparison_sweep \
    --count 16  # 4ê°œ ëª¨ë¸ Ã— 4ë²ˆ ì‹¤í–‰
```

### âš¡ ì¼€ì´ìŠ¤ 4: ë³‘ë ¬ ì‹¤í—˜ ì‹¤í–‰
ì‹œê°„ì„ ì ˆì•½í•˜ê¸° ìœ„í•´ 4ê°œì˜ GPUë¡œ ë™ì‹œ ì‹¤í–‰:

```bash
cd code
python parallel_sweep_runner.py \
    --base-config config/base_config.yaml \
    --single-parallel hyperparameter_sweep \
    --num-workers 4 \
    --runs-per-worker 5
```

---

## ìƒì„¸ ì‚¬ìš©ë²•

### 1. ì„¤ì • íŒŒì¼ êµ¬ì¡° ì´í•´í•˜ê¸°

#### base_config.yaml
```yaml
meta:
  experiment_name: "dialogue_summarization"
  version: "1.0"
  description: "ëŒ€í™” ìš”ì•½ ê¸°ë³¸ ì‹¤í—˜"

general:
  seed: 42
  model_name: "digit82/kobart-summarization"
  output_dir: "./outputs"
  device: "auto"  # auto, cuda, cpu

model:
  architecture: "kobart"  # kobart, kogpt2, t5, mt5
  checkpoint: "digit82/kobart-summarization"
  load_pretrained: true

tokenizer:
  encoder_max_len: 512
  decoder_max_len: 128

training:
  per_device_train_batch_size: 16
  learning_rate: 5e-5
  num_train_epochs: 3
  warmup_ratio: 0.1
  fp16: true  # GPU ë©”ëª¨ë¦¬ ì ˆì•½

generation:
  num_beams: 4
  length_penalty: 1.0
  no_repeat_ngram_size: 2
  max_length: 100

wandb:
  project: "nlp-dialogue-summarization"
  entity: null  # íŒ€ ì´ë¦„
  mode: "online"  # online, offline, disabled
```

### 2. Sweep ì„¤ì • ì»¤ìŠ¤í„°ë§ˆì´ì§•

#### ì˜ˆì‹œ: ë‚˜ë§Œì˜ Sweep ì„¤ì • ë§Œë“¤ê¸°
```yaml
# config/sweep/my_custom_sweep.yaml
name: "Custom Learning Rate Search"
method: "bayes"  # grid, random, bayes
metric:
  name: "best/rouge_combined_f1"
  goal: "maximize"

parameters:
  learning_rate:
    distribution: "log_uniform_values"
    min: 1e-6
    max: 1e-3
  
  warmup_ratio:
    values: [0.0, 0.1, 0.2]
  
  label_smoothing:
    values: [0.0, 0.1, 0.2]

early_terminate:
  type: "hyperband"
  min_iter: 3
```

### 3. ì‹¤í–‰ ìŠ¤í¬ë¦½íŠ¸ í™œìš©

Windows ì‚¬ìš©ìë¥¼ ìœ„í•œ ë°°ì¹˜ íŒŒì¼:
```batch
# scripts/run_hyperparameter_sweep.bat
cd ..
python sweep_runner.py ^
    --base-config config\base_config.yaml ^
    --sweep-config hyperparameter_sweep ^
    --count 50
```

Linux/Mac ì‚¬ìš©ìë¥¼ ìœ„í•œ ì‰˜ ìŠ¤í¬ë¦½íŠ¸:
```bash
# scripts/run_hyperparameter_sweep.sh
#!/bin/bash
python sweep_runner.py \
    --base-config ../config/base_config.yaml \
    --sweep-config hyperparameter_sweep \
    --count 50
```

### 4. ê³ ê¸‰ ê¸°ëŠ¥ í™œìš©

#### ê¸°ì¡´ Sweep ì¬ê°œí•˜ê¸°
```bash
python sweep_runner.py \
    --base-config config/base_config.yaml \
    --sweep-config hyperparameter_sweep \
    --sweep-id "your-sweep-id" \
    --resume \
    --count 20
```

#### í™˜ê²½ë³€ìˆ˜ë¡œ ì„¤ì • ì˜¤ë²„ë¼ì´ë“œ
```bash
export LEARNING_RATE=1e-4
export BATCH_SIZE=8
python trainer.py --config config/base_config.yaml
```

---

## ì‹¤í—˜ ê²°ê³¼ ë¶„ì„

### 1. WandB ëŒ€ì‹œë³´ë“œ í™œìš©

1. [wandb.ai](https://wandb.ai)ì— ì ‘ì†
2. í”„ë¡œì íŠ¸ ì„ íƒ
3. Sweep íƒ­ì—ì„œ ê²°ê³¼ í™•ì¸

ì£¼ìš” í™•ì¸ ì‚¬í•­:
- **Parallel Coordinates Plot**: íŒŒë¼ë¯¸í„° ì¡°í•©ê³¼ ì„±ëŠ¥ì˜ ê´€ê³„
- **Importance Plot**: ê° íŒŒë¼ë¯¸í„°ì˜ ì¤‘ìš”ë„
- **Best Runs**: ìµœê³  ì„±ëŠ¥ ì‹¤í–‰ ëª©ë¡

### 2. ë¡œì»¬ ê²°ê³¼ íŒŒì¼ ë¶„ì„

#### ê²°ê³¼ íŒŒì¼ êµ¬ì¡°
```
outputs/
â”œâ”€â”€ sweep_hyperparameter_sweep/
â”‚   â”œâ”€â”€ sweep_info.json           # Sweep ì •ë³´
â”‚   â”œâ”€â”€ all_sweep_results.jsonl   # ëª¨ë“  ì‹¤í–‰ ê²°ê³¼
â”‚   â””â”€â”€ sweep_summary.json        # ìš”ì•½ í†µê³„
â””â”€â”€ experiments/
    â””â”€â”€ 20240726_143052_a1b2c3d4/
        â”œâ”€â”€ training_results.json  # í•™ìŠµ ê²°ê³¼
        â””â”€â”€ models/best_model/     # ì €ì¥ëœ ëª¨ë¸
```

#### ìµœê³  ì„±ëŠ¥ ëª¨ë¸ ì°¾ê¸°
```python
import json

# Sweep ìš”ì•½ íŒŒì¼ ì½ê¸°
with open('outputs/sweep_hyperparameter_sweep/sweep_summary.json', 'r') as f:
    summary = json.load(f)

print(f"Best ROUGE Combined F1: {summary['best_rouge_combined_f1']:.4f}")
print(f"Best Model Path: {summary['best_model_path']}")
print("\nBest Hyperparameters:")
for param, value in summary['best_params'].items():
    print(f"  {param}: {value}")
```

### 3. ëª¨ë¸ ë¡œë”© ë° ì‚¬ìš©

```python
from transformers import AutoModelForSeq2SeqLM, AutoTokenizer

# ìµœê³  ì„±ëŠ¥ ëª¨ë¸ ë¡œë”©
model_path = "outputs/experiments/.../models/best_model"
model = AutoModelForSeq2SeqLM.from_pretrained(model_path)
tokenizer = AutoTokenizer.from_pretrained(model_path)

# ì¶”ë¡  ì‹¤í–‰
text = "ëŒ€í™” ë‚´ìš©..."
inputs = tokenizer(text, return_tensors="pt", max_length=512, truncation=True)
outputs = model.generate(**inputs, max_length=100, num_beams=4)
summary = tokenizer.decode(outputs[0], skip_special_tokens=True)
```

---

## ë¬¸ì œ í•´ê²°

### ğŸ”§ ì¼ë°˜ì ì¸ ë¬¸ì œì™€ í•´ê²° ë°©ë²•

#### 1. GPU ë©”ëª¨ë¦¬ ë¶€ì¡± (CUDA Out of Memory)

**ì¦ìƒ**: `RuntimeError: CUDA out of memory`

**í•´ê²° ë°©ë²•**:
```yaml
# ë°°ì¹˜ í¬ê¸° ê°ì†Œ
training:
  per_device_train_batch_size: 8  # 16 â†’ 8
  gradient_accumulation_steps: 2   # ì¶”ê°€í•˜ì—¬ íš¨ê³¼ì ì¸ ë°°ì¹˜ í¬ê¸° ìœ ì§€

# FP16 í™œì„±í™”
training:
  fp16: true

# ì‹œí€€ìŠ¤ ê¸¸ì´ ê°ì†Œ
tokenizer:
  encoder_max_len: 256  # 512 â†’ 256
```

#### 2. WandB ì—°ê²° ë¬¸ì œ

**ì¦ìƒ**: `wandb: ERROR Failed to connect to W&B servers`

**í•´ê²° ë°©ë²•**:
```bash
# ì˜¤í”„ë¼ì¸ ëª¨ë“œë¡œ ì‹¤í–‰
export WANDB_MODE=offline

# ë˜ëŠ” configì—ì„œ ì„¤ì •
wandb:
  mode: "offline"
```

#### 3. ëŠë¦° í•™ìŠµ ì†ë„

**ì¦ìƒ**: 1 ì—í­ì— 2ì‹œê°„ ì´ìƒ ì†Œìš”

**í•´ê²° ë°©ë²•**:
```yaml
# ë°ì´í„° ë¡œë” ì›Œì»¤ ì¦ê°€
training:
  dataloader_num_workers: 8

# Mixed precision training
training:
  fp16: true
  fp16_opt_level: "O2"
```

#### 4. ì‹¤í—˜ ì¤‘ë‹¨ í›„ ì¬ì‹œì‘

**ì¦ìƒ**: í•™ìŠµì´ ì¤‘ê°„ì— ì¤‘ë‹¨ë¨

**í•´ê²° ë°©ë²•**:
```bash
# ì²´í¬í¬ì¸íŠ¸ì—ì„œ ì¬ì‹œì‘
python trainer.py \
    --config config/base_config.yaml \
    --resume-from-checkpoint outputs/experiments/.../checkpoints/checkpoint-500
```

### ğŸ› ë””ë²„ê¹… íŒ

1. **ë¡œê·¸ íŒŒì¼ í™•ì¸**
```bash
# ì‹¤í—˜ ë¡œê·¸
cat outputs/experiments/*/training.log

# Sweep ì›Œì»¤ ë¡œê·¸
cat sweep_results/logs/worker_*.log
```

2. **ì„¤ì • ê²€ì¦**
```python
from utils.config_manager import ConfigManager

cm = ConfigManager()
config = cm.load_config("config/base_config.yaml")
print(cm.validate_config_file("config/base_config.yaml"))
```

3. **GPU ì‚¬ìš©ë¥  ëª¨ë‹ˆí„°ë§**
```bash
# ì‹¤ì‹œê°„ GPU ëª¨ë‹ˆí„°ë§
watch -n 1 nvidia-smi
```

---

## ìì£¼ ë¬»ëŠ” ì§ˆë¬¸

### Q1: ê¸°ì¡´ baseline.ipynbë¥¼ ê³„ì† ì‚¬ìš©í•  ìˆ˜ ìˆë‚˜ìš”?
**A**: ë„¤, ì™„ì „íˆ í˜¸í™˜ë©ë‹ˆë‹¤. ì‹¤í—˜ ìë™í™” ì‹œìŠ¤í…œì€ ì„ íƒì ìœ¼ë¡œ ì‚¬ìš©í•  ìˆ˜ ìˆìœ¼ë©°, ê¸°ì¡´ ë…¸íŠ¸ë¶ê³¼ ë³‘í–‰ ì‚¬ìš©ì´ ê°€ëŠ¥í•©ë‹ˆë‹¤.

### Q2: ì–´ë–¤ ìƒí™©ì—ì„œ Sweepì„ ì‚¬ìš©í•´ì•¼ í•˜ë‚˜ìš”?
**A**: 
- ìµœì ì˜ í•˜ì´í¼íŒŒë¼ë¯¸í„°ë¥¼ ì°¾ê³  ì‹¶ì„ ë•Œ
- ì—¬ëŸ¬ ëª¨ë¸ì˜ ì„±ëŠ¥ì„ ê³µì •í•˜ê²Œ ë¹„êµí•˜ê³  ì‹¶ì„ ë•Œ
- ê°™ì€ ì‹¤í—˜ì„ ì—¬ëŸ¬ ì‹œë“œë¡œ ë°˜ë³µí•˜ê³  ì‹¶ì„ ë•Œ

### Q3: WandB ì—†ì´ë„ ì‚¬ìš©í•  ìˆ˜ ìˆë‚˜ìš”?
**A**: ë„¤, ê°€ëŠ¥í•©ë‹ˆë‹¤. configì—ì„œ `wandb.mode: "disabled"`ë¡œ ì„¤ì •í•˜ë©´ ë¡œì»¬ì—ë§Œ ê²°ê³¼ê°€ ì €ì¥ë©ë‹ˆë‹¤.

### Q4: ì»¤ìŠ¤í…€ ë©”íŠ¸ë¦­ì„ ì¶”ê°€í•˜ê³  ì‹¶ì–´ìš”.
**A**: `utils/metrics.py`ì— ë©”íŠ¸ë¦­ì„ ì¶”ê°€í•˜ê³ , `trainer.py`ì˜ `compute_metrics` í•¨ìˆ˜ë¥¼ ìˆ˜ì •í•˜ë©´ ë©ë‹ˆë‹¤.

### Q5: íŠ¹ì • GPUë¥¼ ì§€ì •í•˜ê³  ì‹¶ì–´ìš”.
**A**: í™˜ê²½ë³€ìˆ˜ë¡œ ì„¤ì •í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤:
```bash
export CUDA_VISIBLE_DEVICES=0,1  # GPU 0, 1ë²ˆë§Œ ì‚¬ìš©
```

### Q6: ì‹¤í—˜ ê²°ê³¼ë¥¼ íŒ€ì›ê³¼ ê³µìœ í•˜ê³  ì‹¶ì–´ìš”.
**A**: 
1. WandB íŒ€ ê³„ì • ì‚¬ìš© (ì¶”ì²œ)
2. `outputs/` í´ë”ë¥¼ ì••ì¶•í•˜ì—¬ ê³µìœ 
3. ìµœê³  ì„±ëŠ¥ ëª¨ë¸ë§Œ ê³µìœ : `outputs/experiments/.../models/best_model/`

### Q7: ë©”ëª¨ë¦¬ê°€ ë¶€ì¡±í•œë° í° ëª¨ë¸ì„ ì¨ì•¼ í•´ìš”.
**A**: 
- Gradient checkpointing í™œì„±í™”
- DeepSpeed í†µí•© (í–¥í›„ ì§€ì› ì˜ˆì •)
- ëª¨ë¸ ì–‘ìí™” ì‚¬ìš©

---

## ë‹¤ìŒ ë‹¨ê³„

1. **ê¸°ë³¸ ì‹¤í—˜ ì‹¤í–‰**: ì‹œìŠ¤í…œì— ìµìˆ™í•´ì§€ê¸° ìœ„í•´ ê°„ë‹¨í•œ ì‹¤í—˜ë¶€í„° ì‹œì‘
2. **ì„¤ì • ê°€ì´ë“œ ì½ê¸°**: [configuration_guide.md](configuration_guide.md)ì—ì„œ ìƒì„¸ ì„¤ì • ë°©ë²• í™•ì¸
3. **ë² ìŠ¤íŠ¸ í”„ë™í‹°ìŠ¤ í•™ìŠµ**: [best_practices.md](best_practices.md)ì—ì„œ íš¨ìœ¨ì ì¸ ì‹¤í—˜ ë°©ë²• í™•ì¸
4. **íŒ€ í‘œì¤€ ìˆ˜ë¦½**: íŒ€ ë‚´ ì‹¤í—˜ ëª…ëª… ê·œì¹™, íŒŒë¼ë¯¸í„° ë²”ìœ„ ë“± í‘œì¤€í™”

---

## ê¸°ì—¬í•˜ê¸°

ì‹¤í—˜ ìë™í™” ì‹œìŠ¤í…œ ê°œì„ ì— ê¸°ì—¬í•˜ê³  ì‹¶ë‹¤ë©´:

1. ë²„ê·¸ ë°œê²¬ ì‹œ ì´ìŠˆ ìƒì„±
2. ìƒˆë¡œìš´ ê¸°ëŠ¥ ì œì•ˆ
3. ë¬¸ì„œ ê°œì„ 
4. ì½”ë“œ ê¸°ì—¬ (PR í™˜ì˜!)

---

## ì°¸ê³  ìë£Œ

- [WandB ê³µì‹ ë¬¸ì„œ](https://docs.wandb.ai)
- [Transformers ë¼ì´ë¸ŒëŸ¬ë¦¬ ë¬¸ì„œ](https://huggingface.co/docs/transformers)
- [6ì¡° CV í”„ë¡œì íŠ¸ ë¶„ì„ ë¬¸ì„œ](../6jo_analysis.md)
- [í”„ë¡œì íŠ¸ êµ¬ì¡° í™•ì¥ ì„¤ê³„](../project_extension_design.md)

---

*ì´ ë¬¸ì„œëŠ” ì§€ì†ì ìœ¼ë¡œ ì—…ë°ì´íŠ¸ë©ë‹ˆë‹¤. ìµœì‹  ë²„ì „ì€ í”„ë¡œì íŠ¸ ì €ì¥ì†Œì—ì„œ í™•ì¸í•˜ì„¸ìš”.*
