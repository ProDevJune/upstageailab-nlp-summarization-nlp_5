{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "642f7ab0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/huggingface_hub/file_download.py:943: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "You passed along `num_labels=3` with an incompatible id to label map: {'0': 'NEGATIVE', '1': 'POSITIVE'}. The number of labels wil be overwritten to 2.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, BartForConditionalGeneration, BartConfig\n",
    "model_name = \"digit82/kobart-summarization\"\n",
    "\n",
    "bart_config = BartConfig().from_pretrained(model_name)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = BartForConditionalGeneration.from_pretrained(model_name, config=bart_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "a28bf67c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    BOS : <s>,\n",
      "    EOS : </s>,\n",
      "    Special_tokens : {'bos_token': '<s>', 'eos_token': '</s>', 'unk_token': '<unk>', 'pad_token': '<pad>', 'mask_token': '<mask>'},\n",
      "    Tokenizer's max_model_input_sizes : 1000000000000000019884624838656\n",
      "\n",
      "    BART Config max token length: 1026\n",
      "    BART Embedding Layer: Embedding(30000, 768, padding_idx=3), BartLearnedPositionalEmbedding(1028, 768)\n",
      "    tokenizer의 encoder_max_len은 1028 이하여야 한다.\n",
      "\n",
      "    BART Decoder Layer: Embedding(30000, 768, padding_idx=3), BartLearnedPositionalEmbedding(1028, 768)\n",
      "    tokenizer의 decoder_max_len은 1028 이하여야 한다.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# from pprint import pprint\n",
    "print(f'''\n",
    "    BOS : {tokenizer.bos_token},\n",
    "    EOS : {tokenizer.eos_token},\n",
    "    Special_tokens : {tokenizer.special_tokens_map},\n",
    "    Tokenizer's max_model_input_sizes : {tokenizer.model_max_length}\n",
    "\n",
    "    BART Config max token length: {bart_config.max_position_embeddings}\n",
    "    BART Embedding Layer: {model.get_encoder().embed_tokens}, {model.get_encoder().embed_positions}\n",
    "    tokenizer의 encoder_max_len은 {model.get_encoder().embed_positions.num_embeddings} 이하여야 한다.\n",
    "\n",
    "    BART Decoder Layer: {model.get_decoder().embed_tokens}, {model.get_decoder().embed_positions}\n",
    "    tokenizer의 decoder_max_len은 {model.get_decoder().embed_positions.num_embeddings} 이하여야 한다.\n",
    "\n",
    "''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "b4a9cd0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델의 구성 정보를 YAML 파일로 저장합니다.\n",
    "project_dir = \"/data/ephemeral/home/nlp-5/song/\"\n",
    "import os\n",
    "import sys\n",
    "sys.path.append(\n",
    "    project_dir\n",
    ")\n",
    "from src.utils.utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "514c6d3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0725172301\n"
     ]
    }
   ],
   "source": [
    "current_time = get_current_time()\n",
    "output_dir = f\"./outputs/exp_base_{current_time}\"\n",
    "save_eval_log_steps = 400 # save_step은 eval, log step의 배수여야 한다. 같이 맞춰주는 것이 편하다.\n",
    "print(current_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d13d51eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "config_data = {\n",
    "    \"general\": {\n",
    "        \"data_path\": \"./data/\", # 모델 생성에 필요한 데이터 경로를 사용자 환경에 맞게 지정합니다.\n",
    "        \"model_name\": \"digit82/kobart-summarization\", # 불러올 모델의 이름을 사용자 환경에 맞게 지정할 수 있습니다.\n",
    "        \"output_dir\": output_dir # 모델의 최종 출력 값을 저장할 경로를 설정합니다.\n",
    "    },\n",
    "    \"tokenizer\": {\n",
    "        \"encoder_max_len\": 512,\n",
    "        \"decoder_max_len\": 250,\n",
    "        \"bos_token\": f\"{tokenizer.bos_token}\",\n",
    "        \"eos_token\": f\"{tokenizer.eos_token}\",\n",
    "        # 특정 단어들이 분해되어 tokenization이 수행되지 않도록 special_tokens을 지정해줍니다.\n",
    "        \"special_tokens\": ['#Person1#', '#Person2#', '#Person3#', '#Person4#', '#PhoneNumber#', '#Address#', \n",
    "        '#DateOfBirth#','#PassportNumber#','#SSN#','#CardNumber#','#CarNumber#','#Email#']\n",
    "    },\n",
    "    \"training\": {\n",
    "        \"seed\": 42,\n",
    "        \"output_dir\": output_dir,\n",
    "        \"overwrite_output_dir\": False,\n",
    "\n",
    "        \"save_total_limit\": 2,\n",
    "        \"load_best_model_at_end\": True,\n",
    "        \"save_steps\": save_eval_log_steps,\n",
    "\n",
    "        \"logging_dir\": output_dir,\n",
    "        \"logging_steps\": save_eval_log_steps,\n",
    "\n",
    "        \"num_train_epochs\": 35,\n",
    "        \"per_device_train_batch_size\": 128,\n",
    "        \"remove_unused_columns\": True,\n",
    "        \"fp16\": True,\n",
    "        \"dataloader_drop_last\": False,\n",
    "        \"group_by_length\": True,\n",
    "        \n",
    "        \"gradient_checkpointing\": True,\n",
    "        \"gradient_checkpointing_kwargs\": {\"use_reentrant\": False},\n",
    "        \"gradient_accumulation_steps\": 1,\n",
    "        # \"torch_empty_cache_steps\": 10,\n",
    "        \"dataloader_num_workers\": 8,\n",
    "\n",
    "        \"per_device_eval_batch_size\": 48,\n",
    "        \"evaluation_strategy\": 'steps',\n",
    "        \"eval_steps\": save_eval_log_steps,\n",
    "        \n",
    "        \"predict_with_generate\": True,\n",
    "        \"generation_max_length\": 250,\n",
    "        \n",
    "        # Callbacks\n",
    "        \"early_stopping_patience\": 2,\n",
    "        \"early_stopping_threshold\": 0.001,\n",
    "\n",
    "        # Optimizer\n",
    "        \"learning_rate\": 1e-5,\n",
    "        \"warmup_steps\": 10,\n",
    "        \"weight_decay\": 1e-3,\n",
    "\n",
    "        \"report_to\": \"all\" # (선택) wandb를 사용할 때 설정합니다.\n",
    "    },\n",
    "    # (선택) wandb 홈페이지에 가입하여 얻은 정보를 기반으로 작성합니다.\n",
    "    \"wandb\": {\n",
    "        \"entity\": \"skiersong\", # 팀 실험 시 organization 이름\n",
    "        \"project\": \"nlp-5\",\n",
    "        \"name\": f\"baseline_test1_{current_time}\", # 개별 실험 이름\n",
    "        # \"group\": \"\", # 유사한 실험들은 같은 그룹으로 설정\n",
    "        \"notes\": \"Baseline-test\", # 실험에 대한 추가 설명\n",
    "    },\n",
    "    \"inference\": {\n",
    "        \"ckt_dir\": os.path.join(output_dir, 'best'), # 파인튜닝이 진행된 모델의 checkpoint를 저장할 경로를 설정합니다.\n",
    "        \"result_path\": os.path.join(output_dir, f\"submission_{current_time}.csv\"), # 제출할 csv 파일 저장 경로\n",
    "        \"no_repeat_ngram_size\": 2,\n",
    "        \"early_stopping\": True,\n",
    "        \"generate_max_length\": 200,\n",
    "        \"num_beams\": 4,\n",
    "        \"batch_size\" : 32,\n",
    "        # 정확한 모델 평가를 위해 제거할 불필요한 생성 토큰들을 정의합니다.\n",
    "        \"remove_tokens\": ['<usr>', f\"{tokenizer.bos_token}\", f\"{tokenizer.eos_token}\", f\"{tokenizer.pad_token}\"]\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "616c2f9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'general': {'data_path': './data/',\n",
      "             'model_name': 'digit82/kobart-summarization',\n",
      "             'output_dir': './outputs/exp_base_0725172301'},\n",
      " 'inference': {'batch_size': 32,\n",
      "               'ckt_dir': './outputs/exp_base_0725172301/best',\n",
      "               'early_stopping': True,\n",
      "               'generate_max_length': 200,\n",
      "               'no_repeat_ngram_size': 2,\n",
      "               'num_beams': 4,\n",
      "               'remove_tokens': ['<usr>', '<s>', '</s>', '<pad>'],\n",
      "               'result_path': './outputs/exp_base_0725172301/submission_0725172301.csv'},\n",
      " 'tokenizer': {'bos_token': '<s>',\n",
      "               'decoder_max_len': 250,\n",
      "               'encoder_max_len': 512,\n",
      "               'eos_token': '</s>',\n",
      "               'special_tokens': ['#Person1#',\n",
      "                                  '#Person2#',\n",
      "                                  '#Person3#',\n",
      "                                  '#Person4#',\n",
      "                                  '#PhoneNumber#',\n",
      "                                  '#Address#',\n",
      "                                  '#DateOfBirth#',\n",
      "                                  '#PassportNumber#',\n",
      "                                  '#SSN#',\n",
      "                                  '#CardNumber#',\n",
      "                                  '#CarNumber#',\n",
      "                                  '#Email#']},\n",
      " 'training': {'dataloader_drop_last': False,\n",
      "              'dataloader_num_workers': 8,\n",
      "              'early_stopping_patience': 2,\n",
      "              'early_stopping_threshold': 0.001,\n",
      "              'eval_steps': 400,\n",
      "              'evaluation_strategy': 'steps',\n",
      "              'fp16': True,\n",
      "              'generation_max_length': 250,\n",
      "              'gradient_accumulation_steps': 1,\n",
      "              'gradient_checkpointing': True,\n",
      "              'gradient_checkpointing_kwargs': {'use_reentrant': False},\n",
      "              'group_by_length': True,\n",
      "              'learning_rate': 1e-05,\n",
      "              'load_best_model_at_end': True,\n",
      "              'logging_dir': './outputs/exp_base_0725172301',\n",
      "              'logging_steps': 400,\n",
      "              'num_train_epochs': 35,\n",
      "              'output_dir': './outputs/exp_base_0725172301',\n",
      "              'overwrite_output_dir': False,\n",
      "              'per_device_eval_batch_size': 48,\n",
      "              'per_device_train_batch_size': 128,\n",
      "              'predict_with_generate': True,\n",
      "              'remove_unused_columns': True,\n",
      "              'report_to': 'all',\n",
      "              'save_steps': 400,\n",
      "              'save_total_limit': 2,\n",
      "              'seed': 42,\n",
      "              'warmup_steps': 10,\n",
      "              'weight_decay': 0.001},\n",
      " 'wandb': {'entity': 'skiersong',\n",
      "           'name': 'baseline_test1_0725172301',\n",
      "           'notes': 'Baseline-test',\n",
      "           'project': 'nlp-5'}}\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import yaml\n",
    "from pprint import pprint\n",
    "\n",
    "config_path = os.path.join(\n",
    "    project_dir,'src','configs',\n",
    "    f\"config_base_{current_time}.yaml\" # config 파일 이름을 설정\n",
    ")\n",
    "with open(config_path, \"w\") as file:\n",
    "    yaml.dump(config_data, file, allow_unicode=True)\n",
    "\n",
    "with open(config_path, \"r\") as file:\n",
    "    loaded_config = yaml.safe_load(file)\n",
    "\n",
    "# 불러온 config 파일의 전체 내용을 확인합니다.\n",
    "pprint(loaded_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27711c85",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
